# backend/questionnaire/service.py
from __future__ import annotations

import asyncio
import json
from pathlib import Path
from typing import List, Dict, Optional, Any

from datetime import datetime
from fastapi import HTTPException
from core.providers import providers_from_request

from questionnaire.models import (
    QuestionnaireQuestionModel,
    QuestionBankEntryModel,
    QuestionnaireAnalyzeRequest,
    AnalyzeQuestionnaireResponse,
)
from questionnaire.parser import parse_questions_from_text
from questionnaire.bank import load_question_bank, save_question_bank
from questionnaire.scoring import derive_status_and_confidence
from core.llm_client import (
    call_llm_question_batch,
    call_llm_question_single,
)
from core.config import KNOWLEDGE_STORE_FILE, KNOWLEDGE_DOCS_DIR

BASE_DIR = Path(__file__).resolve().parent.parent

# ---------------------------------------------------------------------
# Similarity scoring
# ---------------------------------------------------------------------


def _question_similarity(a: str, b: str) -> float:
    """
    Very simple token overlap similarity between two question strings.
    """
    a_tokens = set(a.lower().split())
    b_tokens = set(b.lower().split())
    if not a_tokens or not b_tokens:
        return 0.0
    overlap = len(a_tokens & b_tokens)
    return overlap / max(1, len(a_tokens))


def _entry_question_similarity(
    question_text: str, entry: QuestionBankEntryModel
) -> float:
    """
    Compute the best similarity between a question and a bank entry, considering:
      - entry.text (canonical question)
      - entry.variants (optional paraphrases generated by the LLM or fallback)
    """
    best = _question_similarity(question_text, entry.text or "")

    # If this entry has stored variants, check them too
    variants = getattr(entry, "variants", None) or []
    for v in variants:
        score = _question_similarity(question_text, str(v))
        if score > best:
            best = score

    return best


# ---------------------------------------------------------------------
# Robust answer unwrapping
# ---------------------------------------------------------------------


def _extract_plain_answer(raw: Any) -> str:
    """
    Normalize LLM answer text:

    - If it's a JSON object with an "answer" or "answers[0].answer", unwrap it.
    - If it's already plain text, return as-is.
    - If it's text with some intro + JSON (for example: "Here is the response: {...}"),
      try to locate and parse the JSON block, then extract an answer.
    """
    # Case 1: direct dict
    if isinstance(raw, dict):
        if "answer" in raw and isinstance(raw["answer"], str):
            return raw["answer"]
        if "answers" in raw and isinstance(raw["answers"], list) and raw["answers"]:
            first = raw["answers"][0]
            if isinstance(first, dict) and "answer" in first:
                return str(first["answer"])
        # Fallback: stringify dict
        return json.dumps(raw)

    # Normalize to string
    if raw is None:
        return ""
    s = str(raw).strip()
    if not s:
        return ""

    # Helper: try parse JSON and pull out an "answer"
    def try_extract_from_json(text: str) -> Optional[str]:
        txt = text.strip()

        # Strip ``` fences if present
        if txt.startswith("```"):
            txt = txt.strip("`")
            if txt.lower().startswith("json"):
                txt = txt[4:].lstrip()

        try:
            data = json.loads(txt)
        except Exception:
            return None

        if isinstance(data, dict):
            # direct "answer"
            if "answer" in data and isinstance(data["answer"], str):
                return data["answer"]
            # {"answers":[{"answer": "..."}]}
            ans_list = data.get("answers")
            if isinstance(ans_list, list) and ans_list:
                first = ans_list[0]
                if isinstance(first, dict) and "answer" in first:
                    return str(first["answer"])

        # If JSON itself is just a string
        if isinstance(data, str):
            return data

        return None

    # Case 2: string that IS JSON
    if s.startswith("{") or s.startswith("["):
        out = try_extract_from_json(s)
        if out is not None:
            return out

    # Case 3: string with intro + JSON later
    brace_idx = s.find("{")
    bracket_idx = s.find("[") if "[" in s else -1
    json_start = -1
    if brace_idx != -1 and bracket_idx != -1:
        json_start = min(brace_idx, bracket_idx)
    elif brace_idx != -1:
        json_start = brace_idx
    elif bracket_idx != -1:
        json_start = bracket_idx

    if json_start != -1:
        candidate = s[json_start:]
        out = try_extract_from_json(candidate)
        if out is not None:
            return out

    # Case 4: no usable JSON, return original text
    return s


# ---------------------------------------------------------------------
# Tags helper
# ---------------------------------------------------------------------


def _merge_tags(*tag_lists: Optional[List[str]]) -> List[str]:
    """
    Merge multiple tag lists into a unique, ordered list.

    - Ignores None
    - Deduplicates while preserving first-seen order
    - Converts all tags to stripped strings
    """
    seen = set()
    merged: List[str] = []

    for tags in tag_lists:
        if not tags:
            continue
        for t in tags:
            if t is None:
                continue
            s = str(t).strip()
            if not s or s in seen:
                continue
            seen.add(s)
            merged.append(s)

    return merged


# ---------------------------------------------------------------------
# Knowledge store helpers
# ---------------------------------------------------------------------


def _load_knowledge_meta() -> Dict[str, dict]:
    """
    Load knowledge metadata from knowledge_store.json.
    Returns dict keyed by doc_id.
    """
    path = Path(KNOWLEDGE_STORE_FILE)
    if not path.exists():
        return {}

    try:
        raw = path.read_text(encoding="utf-8")
        data = json.loads(raw)
    except Exception:
        return {}

    meta_by_id: Dict[str, dict] = {}
    if isinstance(data, list):
        for item in data:
            if isinstance(item, dict):
                doc_id = str(item.get("id") or "")
                if doc_id:
                    meta_by_id[doc_id] = item
    return meta_by_id


def build_knowledge_context_from_ids(
    knowledge_doc_ids: Optional[List[str]],
    max_chars_per_doc: int = 6000,
) -> tuple[str, Dict[str, dict]]:
    """
    Given selected knowledge_doc_ids, build:
      - combined context string (for LLM)
      - dict of knowledge_sources metadata per id:
        {
          "doc_id": "...",
          "title": "...",
          "doc_type": "...",
          "tags": [...],
          "snippet": "..."
        }
    """
    if not knowledge_doc_ids:
        return "", {}

    meta_by_id = _load_knowledge_meta()
    chunks: List[str] = []
    sources: Dict[str, dict] = {}

    for doc_id in knowledge_doc_ids:
        txt_path = Path(KNOWLEDGE_DOCS_DIR) / f"{doc_id}.txt"
        if not txt_path.exists():
            continue

        try:
            text = txt_path.read_text(encoding="utf-8", errors="ignore").strip()
        except Exception:
            continue

        if not text:
            continue

        snippet_for_llm = text[:max_chars_per_doc]
        snippet_for_ui = text[:500]

        meta = meta_by_id.get(doc_id, {})
        title = meta.get("title") or doc_id
        doc_type = meta.get("doc_type")
        tags = meta.get("tags") or []

        chunks.append(f"[{title}]\n{snippet_for_llm}")

        sources[doc_id] = {
            "doc_id": doc_id,
            "title": title,
            "doc_type": doc_type,
            "tags": tags,
            "snippet": snippet_for_ui,
        }

    context = "\n\n".join(chunks)
    return context, sources


# ---------------------------------------------------------------------
# Main analysis
# ---------------------------------------------------------------------


async def analyze_questionnaire(
    body: QuestionnaireAnalyzeRequest,
) -> AnalyzeQuestionnaireResponse:
    # Defensive handling + logging
    raw = (body.raw_text or "").strip()
    llm_enabled = getattr(body, "llm_enabled", True)
    knowledge_doc_ids = getattr(body, "knowledge_doc_ids", None) or []

    print(
        "[QUESTIONNAIRE] analyze_questionnaire: start "
        f"raw_len={len(raw)}, llm_enabled={llm_enabled}, "
        f"knowledge_doc_ids={knowledge_doc_ids}"
    )

    if not raw:
        raise HTTPException(status_code=400, detail="raw_text must be non-empty.")

    questions = parse_questions_from_text(raw)
    print(
        "[QUESTIONNAIRE] analyze_questionnaire: parsed "
        f"{len(questions)} questions from raw_text"
    )

    if not questions:
        print(
            "[QUESTIONNAIRE] analyze_questionnaire: no questions parsed, "
            "returning early"
        )
        return AnalyzeQuestionnaireResponse(
            raw_text=raw,
            questions=[],
            overall_confidence=None,
        )

    bank_entries = load_question_bank(storage)
    print(
        "[QUESTIONNAIRE] analyze_questionnaire: loaded "
        f"{len(bank_entries)} bank entries"
    )

    BANK_STRONG = 0.70
    BANK_WEAK = 0.40

    best_map: Dict[str, Dict[str, Any]] = {}
    remaining_for_llm: List[QuestionnaireQuestionModel] = []

    # -----------------------------------------------------------------
    # Pass 1: bank matching
    # -----------------------------------------------------------------
    for q in questions:
        best_entry: Optional[QuestionBankEntryModel] = None
        best_score: float = 0.0

        for entry in bank_entries:
            score = _entry_question_similarity(q.question_text, entry)
            if score > best_score:
                best_score = score
                best_entry = entry

        best_map[q.id] = {"best_entry": best_entry, "best_score": best_score}

        # Collect tags from best_entry (if any)
        bank_tags: List[str] = []
        if best_entry:
            if best_entry.primary_tag:
                bank_tags.append(best_entry.primary_tag)
            if best_entry.frameworks:
                bank_tags.extend(best_entry.frameworks)

        # Strong match → use bank only, no LLM
        if best_entry and best_score >= BANK_STRONG:
            q.suggested_answer = best_entry.answer
            q.answer_source = "bank"
            q.matched_bank_id = best_entry.id
            q.confidence = 1.0
            q.tags = _merge_tags(q.tags, bank_tags)

            best_entry.usage_count += 1
            best_entry.last_used_at = datetime.utcnow().isoformat() + "Z"
            continue

        # Weak match but LLM disabled → just link bank and tags
        if not llm_enabled:
            if best_entry and best_score >= BANK_WEAK:
                q.matched_bank_id = best_entry.id
                q.tags = _merge_tags(q.tags, bank_tags)
            continue

        # Otherwise we’ll send this one to LLM
        remaining_for_llm.append(q)

    print(
        "[QUESTIONNAIRE] analyze_questionnaire: "
        f"{len(remaining_for_llm)} questions remaining for LLM; "
        f"llm_enabled={llm_enabled}"
    )

    # Persist bank usage changes
    save_question_bank(storage, bank_entries)

    # -----------------------------------------------------------------
    # Pass 2: batch LLM (best-effort with timeout)
    # -----------------------------------------------------------------
    knowledge_context, knowledge_sources_meta = build_knowledge_context_from_ids(
        knowledge_doc_ids
    )

    unanswered: List[QuestionnaireQuestionModel] = []

    if remaining_for_llm and llm_enabled:
        questions_payload: List[dict] = []

        for q in remaining_for_llm:
            meta = best_map.get(q.id, {})
            best_entry = meta.get("best_entry")
            best_score = meta.get("best_score", 0.0)

            positive = [
                {
                    "id": e.id,
                    "question": e.text,
                    "answer": e.answer,
                    "why_good": "Approved answer from bank.",
                }
                for e in bank_entries
                if e.status == "approved"
            ]

            negative = [
                {
                    "id": e.id,
                    "question": e.text,
                    "answer": e.answer,
                    "reasons": e.rejection_reasons,
                }
                for e in bank_entries
                if e.rejection_reasons or e.status == "retired"
            ]

            similar: List[dict] = []
            if best_entry and best_score >= BANK_WEAK:
                similar.append(
                    {
                        "id": best_entry.id,
                        "question": best_entry.text,
                        "answer": best_entry.answer,
                        "status": best_entry.status,
                        "rejection_reasons": best_entry.rejection_reasons,
                    }
                )

            questions_payload.append(
                {
                    "id": q.id,
                    "question": q.question_text,
                    "positive_examples": positive,
                    "negative_examples": negative,
                    "similar_bank_entries": similar,
                }
            )

        print(
            "[QUESTIONNAIRE] analyze_questionnaire: "
            f"calling batch LLM for {len(questions_payload)} questions"
        )

        try:
            batch_answers: Dict[str, dict] = await asyncio.wait_for(
                call_llm_question_batch(
                    questions_payload=questions_payload,
                    knowledge_context=knowledge_context,
                ),
                timeout=30.0,  # hard cap so the endpoint cannot hang forever
            )
            print(
                "[QUESTIONNAIRE] analyze_questionnaire: "
                f"batch LLM returned answers for {len(batch_answers)} questions"
            )
        except asyncio.TimeoutError:
            print(
                "[QUESTIONNAIRE] Batch LLM timed out after 30s; "
                "falling back to no batch answers"
            )
            unanswered = list(remaining_for_llm)
        except HTTPException as exc:
            print("[QUESTIONNAIRE] Batch LLM HTTPException:", exc.detail)
            unanswered = list(remaining_for_llm)
        except Exception as exc:
            print("[QUESTIONNAIRE] Batch LLM unexpected error:", repr(exc))
            unanswered = list(remaining_for_llm)
        else:
            for q in remaining_for_llm:
                ans_info = batch_answers.get(q.id)
                if not ans_info:
                    unanswered.append(q)
                    continue

                raw_ans = ans_info.get("answer")
                q.suggested_answer = _extract_plain_answer(raw_ans)
                q.answer_source = "llm"

                try:
                    conf = float(ans_info.get("confidence", 0.6))
                except Exception:
                    conf = 0.6
                q.confidence = max(0.0, min(conf, 1.0))

                meta = best_map.get(q.id, {})
                best_entry = meta.get("best_entry")
                best_score = meta.get("best_score", 0.0)

                if best_entry and best_score >= BANK_WEAK:
                    q.matched_bank_id = best_entry.id
                    bank_tags: List[str] = []
                    if best_entry.primary_tag:
                        bank_tags.append(best_entry.primary_tag)
                    if best_entry.frameworks:
                        bank_tags.extend(best_entry.frameworks)
                    q.tags = _merge_tags(q.tags, bank_tags)

                inferred_tags = ans_info.get("inferred_tags")
                if isinstance(inferred_tags, list):
                    cleaned = [str(t).strip() for t in inferred_tags if t]
                    q.tags = _merge_tags(q.tags, cleaned)

                if knowledge_sources_meta:
                    q.knowledge_sources = list(knowledge_sources_meta.values())

    # -----------------------------------------------------------------
    # Pass 3: per-question LLM fallback (with timeout)
    # -----------------------------------------------------------------
    if unanswered and llm_enabled:
        print(
            "[QUESTIONNAIRE] analyze_questionnaire: "
            f"running per-question fallback for {len(unanswered)} questions"
        )

    for q in unanswered:
        meta = best_map.get(q.id, {})
        best_entry = meta.get("best_entry")
        best_score = meta.get("best_score", 0.0)

        similar_entries: List[QuestionBankEntryModel] = []
        if best_entry and best_score >= BANK_WEAK:
            similar_entries.append(best_entry)

        try:
            ans = await asyncio.wait_for(
                call_llm_question_single(
                    question=q.question_text,
                    similar_bank_entries=similar_entries,
                ),
                timeout=15.0,
            )
        except asyncio.TimeoutError:
            print(f"[QUESTIONNAIRE] Per-question LLM timeout for {q.id}")
            ans = None
        except HTTPException as exc:
            print("[QUESTIONNAIRE] Per-question LLM HTTPException:", exc.detail)
            ans = None
        except Exception as exc:
            print("[QUESTIONNAIRE] Per-question LLM unexpected error:", repr(exc))
            ans = None

        if ans:
            q.suggested_answer = _extract_plain_answer(ans)
            q.answer_source = "llm"
            q.confidence = 0.55

            if best_entry and best_score >= BANK_WEAK:
                bank_tags: List[str] = []
                if best_entry.primary_tag:
                    bank_tags.append(best_entry.primary_tag)
                if best_entry.frameworks:
                    bank_tags.extend(best_entry.frameworks)
                q.tags = _merge_tags(q.tags, bank_tags)

            if knowledge_sources_meta:
                q.knowledge_sources = list(knowledge_sources_meta.values())
        else:
            q.suggested_answer = None
            q.answer_source = None
            q.confidence = None

    overall = derive_status_and_confidence(questions)

    print(
        "[QUESTIONNAIRE] analyze_questionnaire: done "
        f"returning {len(questions)} questions, overall_confidence={overall}"
    )

    return AnalyzeQuestionnaireResponse(
        raw_text=raw,
        questions=questions,
        overall_confidence=overall,
    )



